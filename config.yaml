# Pygame settings
screen_width: 400 # 游戏区域宽度 (10 blocks * 30 pixels) + 右侧信息区
screen_height: 660 # 游戏区域高度 (20 blocks * 30 pixels) + 上下边距
board_width: 10 # 游戏板宽度（格子数）
board_height: 20 # 游戏板高度（格子数）
block_size: 30 # 每个格子的像素大小
info_panel_width: 200 # 右侧信息面板宽度
fps: 30


# <<< 新增：AI游戏模式的可视化回放参数 >>>
ai_playback:
  enabled: true                    # << 总开关：true为像人一样操作，false为瞬移模式
  move_delay_ms: 1                # << 参数1: AI执行每次左/右/旋转动作后的延迟（毫秒）
  auto_fall_interval_ms: 500       # << 参数2: AI每隔多少毫秒自动下落一格
  record_game:
    enabled: false
    output_format: "mp4"
    output_fps: 24
    capture_interval: 2
    record_duration_seconds: 60  # << 新增/修改：直接设置想要的录像总秒数。设为0则不限制。


# Tetrominoes (colors will be defined in code)
# Shapes are defined in tetrominoes.py

# DQN Agent parameters (CNN Version)
network_type: "cnn" # Specify network type: 'mlp' or 'cnn'
cnn_input_channels: 4 # Number of channels for the CNN input
gamma: 0.99 # Discount factor
epsilon_start: 1.0
epsilon_end: 0.01
epsilon_decay_frames: 10000000 # CNN might need longer to learn
batch_size: 64 # Larger batch size can be stable for CNNs
memory_size: 100000 # Increased memory size
target_update_freq: 2500 # Update target network less frequently

# Training parameters
num_episodes: 3000000
learning_rate: 0.0001
max_steps_per_episode: 1000 # Max moves per game episode during training
save_model_freq: 10000 # Episodes
eval_freq: 30000 # Episodes (how often to run an evaluation game with rendering/screenshot)
eval_step_wait_ms: 1
log_freq: 100 # Episodes (how often to print training stats)

# --- Auxiliary Learning Parameters ---
use_auxiliary_heads: true
aux_loss_weight_lines: 0.3    # Weight for the 'completed lines' prediction task
aux_loss_weight_holes: 0.2    # Weight for the 'holes' prediction task
aux_loss_weight_height: 0.1   # Weight for the 'aggregate height' prediction task


# --- Prioritized Experience Replay (PER) Parameters ---
per_epsilon: 0.01  # Small constant to ensure no priority is zero
per_alpha: 0.6     # Controls how much prioritization is used (0=random, 1=full)
per_beta_start: 0.4 # Initial value for importance sampling, anneals to 1.0
per_beta_frames: 1000000 # Number of frames over which to anneal beta

# Paths
weights_dir: "weights/"
screenshots_dir: "screenshots/"
logs_dir: "logs/"

# Game Mechanics
# config.yaml

# ... (其他配置) ...

# Game Mechanics
clear_line_scores: [0, 100, 200, 300, 400] # << 修改: 更新为新的消行基础分
combo_score_base: 100                     # << 新增/修改: 连击基础分 (您提到的 combo_bouns)
combo_multiplier_schedule:                # << 新增: 连击得分的乘数表
  - 0  # Combo 1 (第一次消行)
  - 1  # Combo 2
  - 1  # Combo 3
  - 1  # Combo 4
  - 2  # Combo 5
  - 2  # Combo 6
  - 2  # Combo 7
  - 3  # Combo 8
  - 3  # Combo 9
  - 3  # Combo 10+ (之后的都按3倍计算)

# Reward Weights (Heuristic part, for T-Spin focus)
# Note: With CNN, we rely more on score, but explicit rewards can guide learning.
game_over_penalty: -500 # Heavier penalty for game over
line_clear_reward: 100 # Base reward per line cleared
tetris_reward_bonus: 400 # Bonus for clearing 4 lines at once

# --- T-Spin Specific Rewards ---
# These are crucial for teaching the AI the value of T-Spins
tspin_reward: 400          # Base reward for any successful T-Spin
tspin_mini_reward: 30     # T-Spin Mini (no lines)
tspin_single_reward: 800   # T-Spin Single
tspin_double_reward: 1200  # T-Spin Double (very high value)
tspin_triple_reward: 1600  # T-Spin Triple (ultimate goal)
# ... (其他奖励/惩罚因子可以保留，以便我们切换回“启发式”奖励模式) ...

# # Reward Weights (Heuristic part of the reward beyond score change)
# height_penalty_factor: -0.1
# hole_penalty_factor: -2.5 # Original prompt value, consider making it more negative like -50 or -100
# bumpiness_penalty_factor: -0.8 # Original prompt value, consider -5 or -10
# lines_cleared_reward_factor: 200
# game_over_penalty: -120
# piece_drop_reward: -0.01 # Small positive reward for successfully placing a piece without game over
# combo_base_reward: 200
# well_occupancy_penalty_factor: -8.0 # <<< 新增: 井区占有惩罚因子 (可调整)

feature_scaling_factors:
  max_height: 20.0
  max_holes: 50.0  # 根据观察，空洞数可能很多，这个值可以调整
  max_generalized_wells: 300.0 # 根据观察调整
  max_bumpiness: 50.0 # 根据观察调整
  max_completed_lines: 4.0
  max_combo: 15.0 # 15连击已经非常高了
  max_well_occupancy: 40.0 # <<< 新增: 井区最大占有格子数 (2列 * 20行 = 40)

auto_load_latest_weights: true